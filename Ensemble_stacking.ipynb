{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vecstack import stacking\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import RandomizedLasso\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import LassoLars\n",
    "from sklearn.linear_model import BayesianRidge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step I. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('X_test.csv')\n",
    "\n",
    "X_train = train.iloc[:, :-1]\n",
    "y_train = train.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step II. Perform stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelfit(reg, train, target, performCV=True, cv=10):\n",
    "    reg.fit(train, target)\n",
    "    pred = reg.predict(train)\n",
    "    train_r2_score = r2_score(target, pred)\n",
    "    \n",
    "    print('\\nModel Report')\n",
    "    print('Train_coef_of_det: %0.6f' %(train_r2_score))\n",
    "    \n",
    "    if performCV:\n",
    "        cv_r2_score = cross_val_score(reg, train, target, cv=cv, scoring='r2', n_jobs=-1)\n",
    "        print('CV_coef_of_det: Mean-%0.6f | Std-%0.6f | Min-%0.6f | Max-%0.6f' %(np.mean(cv_r2_score),\n",
    "            np.std(cv_r2_score), np.min(cv_r2_score), np.max(cv_r2_score)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(train, test):\n",
    "    scaler = StandardScaler().fit(train)\n",
    "    train = pd.DataFrame(scaler.transform(train), columns=train.columns)\n",
    "    test = pd.DataFrame(scaler.transform(test), columns=train.columns)\n",
    "    return (train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'reg_alpha': 0.2488306517953651, 'colsample_bylevel': 0.408428905857428,\n",
    "          'scale_pos_weight': 0.9984620415066895, 'learning_rate': 0.1, 'max_delta_step': 0, 'base_score': 0.5,\n",
    "          'n_estimators': 1070, 'seed': 10, 'colsample_bytree': 0.9489607182872746, 'silent': False, 'missing': None,\n",
    "          'nthread': -1, 'min_child_weight': 0.3, 'subsample': 0.9844783382327149, 'reg_lambda': 0.9993727646886654,\n",
    "          'objective': 'reg:linear', 'max_depth': 3, 'gamma': 2.5777502101292433}\n",
    "reg1 = XGBRegressor(\n",
    "                    learning_rate = params['learning_rate'],\n",
    "                    n_estimators = params['n_estimators'],\n",
    "                    max_depth = params['max_depth'],\n",
    "                    gamma = params['gamma'],                        \n",
    "                    max_delta_step = params['max_delta_step'],\n",
    "                    min_child_weight = params['min_child_weight'],\n",
    "                    subsample = params['subsample'],\n",
    "                    colsample_bytree = params['colsample_bytree'],\n",
    "                    colsample_bylevel = params['colsample_bylevel'],\n",
    "                    reg_alpha = params['reg_alpha'],\n",
    "                    reg_lambda = params['reg_lambda'],\n",
    "                    scale_pos_weight = params['scale_pos_weight'],\n",
    "                    base_score = params['base_score'],\n",
    "                    objective = params['objective'],\n",
    "                    silent = params['silent'],\n",
    "                    nthread = params['nthread'],\n",
    "                    seed = params['seed'],\n",
    "                    missing = params['missing']                  \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'warm_start': True, 'verbose': 0, 'max_leaf_nodes': None, 'learning_rate': 0.1, 'min_samples_leaf': 2,\n",
    "          'n_estimators': 220, 'max_features': 0.9827404178094029, 'alpha': 0.16846758069342863, 'presort': True,\n",
    "          'loss': 'ls', 'min_impurity_split': 1e-07, 'subsample': 0.9800365433265062, 'init': None,\n",
    "          'min_weight_fraction_leaf': 0.000546805267271948, 'criterion': 'friedman_mse', 'random_state': 10,\n",
    "          'min_samples_split': 9, 'max_depth': 3}\n",
    "reg2 = GradientBoostingRegressor(\n",
    "                                learning_rate = params['learning_rate'],\n",
    "                                n_estimators = params['n_estimators'],\n",
    "                                max_depth = params['max_depth'],\n",
    "                                subsample = params['subsample'],\n",
    "                                max_features = params['max_features'],\n",
    "                                alpha = params['alpha'],\n",
    "                                min_samples_split = params['min_samples_split'],\n",
    "                                min_samples_leaf = params['min_samples_leaf'],\n",
    "                                min_weight_fraction_leaf = params['min_weight_fraction_leaf'],\n",
    "                                min_impurity_split = params['min_impurity_split'],\n",
    "                                presort = params['presort'],\n",
    "                                loss = params['loss'],\n",
    "                                criterion = params['criterion'],\n",
    "                                init = params['init'],\n",
    "                                verbose = params['verbose'],\n",
    "                                max_leaf_nodes = params['max_leaf_nodes'],\n",
    "                                warm_start = params['warm_start'],\n",
    "                                random_state = params['random_state']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'warm_start': True, 'oob_score': False, 'n_jobs': -1, 'verbose': 0, 'max_leaf_nodes': None,\n",
    "          'bootstrap': False, 'min_samples_leaf': 1, 'n_estimators': 140, 'max_features': 0.7,\n",
    "          'min_weight_fraction_leaf': 0.0, 'criterion': 'mse', 'random_state': 10, 'min_impurity_split': 0,\n",
    "          'min_samples_split': 3, 'max_depth': None}\n",
    "reg3 = ExtraTreesRegressor(\n",
    "                            n_estimators = params['n_estimators'],\n",
    "                            max_depth = params['max_depth'],\n",
    "                            max_features = params['max_features'],\n",
    "                            min_samples_split = params['min_samples_split'],\n",
    "                            min_samples_leaf = params['min_samples_leaf'],\n",
    "                            min_weight_fraction_leaf = params['min_weight_fraction_leaf'],\n",
    "                            min_impurity_split = params['min_impurity_split'],\n",
    "                            bootstrap = params['bootstrap'],\n",
    "                            oob_score = params['oob_score'],\n",
    "                            max_leaf_nodes = params['max_leaf_nodes'],\n",
    "                            criterion = params['criterion'],\n",
    "                            n_jobs = params['n_jobs'],\n",
    "                            warm_start = params['warm_start'],\n",
    "                            verbose = params['verbose'],\n",
    "                            random_state = params['random_state']\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg4 = RandomForestRegressor(n_estimators=70, max_depth=None, max_features=0.52,\n",
    "        min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, min_impurity_split=1e-07, \n",
    "        bootstrap=True, criterion='mse', oob_score=False, max_leaf_nodes=None, n_jobs=-1,\n",
    "        verbose=0, warm_start=False, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'warm_start': True, 'max_samples': 0.9030810827110213, 'base_estimator': None, 'n_jobs': 1,\n",
    "          'verbose': 0, 'bootstrap': False, 'oob_score': False, 'n_estimators': 110, 'random_state': 10,\n",
    "          'max_features': 0.5351477142010489, 'bootstrap_features': False}\n",
    "reg5 = BaggingRegressor(\n",
    "                        n_estimators = params['n_estimators'],\n",
    "                        max_samples = params['max_samples'],\n",
    "                        max_features = params['max_features'],\n",
    "                        bootstrap = params['bootstrap'],\n",
    "                        bootstrap_features = params['bootstrap_features'],\n",
    "                        oob_score = params['oob_score'],\n",
    "                        base_estimator = params['base_estimator'],\n",
    "                        warm_start = params['warm_start'],\n",
    "                        verbose = params['verbose'],\n",
    "                        n_jobs = params['n_jobs'],\n",
    "                        random_state = params['random_state']  \n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'n_estimators': 517, 'loss': 'exponential', 'base_estimator': None, 'random_state': 10,\n",
    "          'learning_rate': 0.1}\n",
    "reg6 = AdaBoostRegressor(\n",
    "                        learning_rate = params['learning_rate'],\n",
    "                        n_estimators = params['n_estimators'],\n",
    "                        loss = params['loss'],\n",
    "                        base_estimator = params['base_estimator'],\n",
    "                        random_state = params['random_state']\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'normalize': False, 'warm_start': True, 'selection': 'cyclic', 'fit_intercept': True,\n",
    "          'l1_ratio': 0.4779542082285552, 'max_iter': 25, 'precompute': True, 'random_state': 10,\n",
    "          'tol': 0.0010509356563245678, 'positive': False, 'copy_X': False, 'alpha': 0.840377112882823}\n",
    "reg7 = ElasticNet(\n",
    "                    alpha = params['alpha'],\n",
    "                    l1_ratio = params['l1_ratio'],\n",
    "                    normalize = params['normalize'],\n",
    "                    precompute = params['precompute'],\n",
    "                    max_iter = params['max_iter'],\n",
    "                    tol = params['tol'],\n",
    "                    positive = params['positive'],\n",
    "                    selection = params['selection'],\n",
    "                    warm_start = params['warm_start'],\n",
    "                    random_state = params['random_state']   \n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'normalize': False, 'n_iter': 7, 'verbose': True, 'lambda_2': 0.004900097067640188,\n",
    "          'fit_intercept': True, 'compute_score': False, 'alpha_2': 0.010070176141440748, 'tol': 0.4475451681770408,\n",
    "          'alpha_1': 5.828472650020041e-12, 'copy_X': True, 'lambda_1': 0.0011557992166094178}\n",
    "reg8 = BayesianRidge(\n",
    "                    alpha_1 = params['alpha_1'],\n",
    "                    alpha_2 = params['alpha_2'],\n",
    "                    lambda_1 = params['lambda_1'],\n",
    "                    lambda_2 = params['lambda_2'],\n",
    "                    n_iter = params['n_iter'],\n",
    "                    tol = params['tol'],\n",
    "                    compute_score = params['compute_score'],\n",
    "                    normalize = params['normalize'],\n",
    "                    fit_intercept = params['fit_intercept'],\n",
    "                    copy_X = params['copy_X'],\n",
    "                    verbose = params['verbose']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'normalize': True, 'warm_start': False, 'selection': 'random', 'fit_intercept': True, 'positive': True,\n",
    "          'max_iter': 17, 'precompute': False, 'random_state': 10, 'tol': 0.046776741099351975, 'copy_X': True,\n",
    "          'alpha': 0.9933646228467098}\n",
    "reg9 = Lasso(\n",
    "            alpha = params['alpha'],\n",
    "            max_iter = params['max_iter'],\n",
    "            tol = params['tol'],\n",
    "            normalize = params['normalize'],\n",
    "            precompute = params['precompute'],\n",
    "            positive = params['positive'],\n",
    "            selection = params['selection'],\n",
    "            fit_intercept = params['fit_intercept'],\n",
    "            copy_X = params['copy_X'],\n",
    "            warm_start = params['warm_start'],\n",
    "            random_state = params['random_state']\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'normalize': False, 'alphas': None, 'n_jobs': -1, 'verbose': False, 'positive': True, 'n_alphas': 2,\n",
    "          'max_iter': 14, 'eps': 0.019137796428146392, 'precompute': True, 'random_state': 10,\n",
    "          'tol': 0.05672233404835849, 'selection': 'cyclic', 'cv': 10}\n",
    "reg10 = LassoCV(\n",
    "            eps = params['eps'],\n",
    "            n_alphas = params['n_alphas'],\n",
    "            max_iter = params['max_iter'],\n",
    "            tol = params['tol'],\n",
    "            normalize = params['normalize'],\n",
    "            positive = params['positive'],\n",
    "            precompute = params['precompute'],\n",
    "            selection = params['selection'],\n",
    "            alphas = params['alphas'],\n",
    "            cv = params['cv'],\n",
    "            n_jobs = params['n_jobs'],\n",
    "            verbose = params['verbose'],\n",
    "            random_state = params['random_state']       \n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n            alpha = params['alpha'],\\n            max_iter = params['max_iter'],\\n            eps = params['eps'],\\n            normalize = params['normalize'],\\n            precompute = params['precompute'],\\n            fit_path = params['fit_path'],\\n            positive = params['positive'],\\n            fit_intercept = params['fit_intercept'],\\n            copy_X = params['copy_X'],\\n            verbose = params['verbose']          \\n            )\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'normalize': False, 'fit_path': False, 'fit_intercept': True, 'positive': True, 'max_iter': 116,\n",
    "          'eps': 0.9766195535161404, 'precompute': 'auto', 'copy_X': True, 'alpha': 0.44414494003332533,\n",
    "          'verbose': False}\n",
    "reg11 = LassoLars()\n",
    "'''\n",
    "            alpha = params['alpha'],\n",
    "            max_iter = params['max_iter'],\n",
    "            eps = params['eps'],\n",
    "            normalize = params['normalize'],\n",
    "            precompute = params['precompute'],\n",
    "            fit_path = params['fit_path'],\n",
    "            positive = params['positive'],\n",
    "            fit_intercept = params['fit_intercept'],\n",
    "            copy_X = params['copy_X'],\n",
    "            verbose = params['verbose']          \n",
    "            )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n            alpha = params['alpha'],\\n            tol = params['tol'],\\n            solver = params['solver'],\\n            max_iter = params['max_iter'],\\n            normalize = params['normalize'], \\n            fit_intercept = params['fit_intercept'],\\n            copy_X = params['copy_X'],\\n            random_state = params['random_state']         \\n            )\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'normalize': True, 'fit_intercept': True, 'max_iter': 44, 'random_state': 10, 'tol': 0.13201162619710619,\n",
    "          'copy_X': True, 'alpha': 0.2815809154477012, 'solver': 'sag'}\n",
    "reg12 = Ridge()\n",
    "'''\n",
    "            alpha = params['alpha'],\n",
    "            tol = params['tol'],\n",
    "            solver = params['solver'],\n",
    "            max_iter = params['max_iter'],\n",
    "            normalize = params['normalize'], \n",
    "            fit_intercept = params['fit_intercept'],\n",
    "            copy_X = params['copy_X'],\n",
    "            random_state = params['random_state']         \n",
    "            )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_models = [reg1, reg2, reg3, reg4, reg5, reg6, reg7, reg8, reg9, reg10, reg11, reg12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute stacking features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:   [regression]\n",
      "metric: [r2_score]\n",
      "\n",
      "model 0: [XGBRegressor]\n",
      "    fold 0: [0.90896434]\n",
      "    fold 1: [0.90402461]\n",
      "    fold 2: [0.89997364]\n",
      "    fold 3: [0.93228209]\n",
      "    fold 4: [0.90538117]\n",
      "    fold 5: [0.92757486]\n",
      "    fold 6: [0.86161071]\n",
      "    fold 7: [0.70196644]\n",
      "    fold 8: [0.89696104]\n",
      "    fold 9: [0.89857591]\n",
      "    ----\n",
      "    MEAN:   [0.88985104]\n",
      "\n",
      "model 1: [GradientBoostingRegressor]\n",
      "    fold 0: [0.90604988]\n",
      "    fold 1: [0.97741298]\n",
      "    fold 2: [0.97502524]\n",
      "    fold 3: [0.98749939]\n",
      "    fold 4: [0.97835686]\n",
      "    fold 5: [0.98791256]\n",
      "    fold 6: [0.98192464]\n",
      "    fold 7: [0.97779665]\n",
      "    fold 8: [0.98617421]\n",
      "    fold 9: [0.97378918]\n",
      "    ----\n",
      "    MEAN:   [0.97515872]\n",
      "\n",
      "model 2: [ExtraTreesRegressor]\n",
      "    fold 0: [0.90194822]\n",
      "    fold 1: [0.99976114]\n",
      "    fold 2: [0.99988630]\n",
      "    fold 3: [0.99971979]\n",
      "    fold 4: [0.99965527]\n",
      "    fold 5: [0.99970676]\n",
      "    fold 6: [0.99974496]\n",
      "    fold 7: [0.99977908]\n",
      "    fold 8: [0.99965918]\n",
      "    fold 9: [0.99966845]\n",
      "    ----\n",
      "    MEAN:   [0.99134175]\n",
      "\n",
      "model 3: [RandomForestRegressor]\n",
      "    fold 0: [0.86759103]\n",
      "    fold 1: [0.88907445]\n",
      "    fold 2: [0.88727105]\n",
      "    fold 3: [0.90143315]\n",
      "    fold 4: [0.85102480]\n",
      "    fold 5: [0.87829590]\n",
      "    fold 6: [0.81858919]\n",
      "    fold 7: [0.72165270]\n",
      "    fold 8: [0.88224656]\n",
      "    fold 9: [0.88459265]\n",
      "    ----\n",
      "    MEAN:   [0.86412594]\n",
      "\n",
      "model 4: [BaggingRegressor]\n",
      "    fold 0: [0.88924769]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:303: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/ensemble/bagging.py:346: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\"Warm-start fitting without increasing n_estimators does not \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 1: [0.99866691]\n",
      "    fold 2: [0.99807115]\n",
      "    fold 3: [0.99878555]\n",
      "    fold 4: [0.99843933]\n",
      "    fold 5: [0.99906506]\n",
      "    fold 6: [0.99796768]\n",
      "    fold 7: [0.99652284]\n",
      "    fold 8: [0.99843837]\n",
      "    fold 9: [0.99851263]\n",
      "    ----\n",
      "    MEAN:   [0.98900714]\n",
      "\n",
      "model 5: [AdaBoostRegressor]\n",
      "    fold 0: [0.83926228]\n",
      "    fold 1: [0.82361600]\n",
      "    fold 2: [0.81117718]\n",
      "    fold 3: [0.88059038]\n",
      "    fold 4: [0.77589038]\n",
      "    fold 5: [0.81446006]\n",
      "    fold 6: [0.81027561]\n",
      "    fold 7: [0.66767998]\n",
      "    fold 8: [0.84100397]\n",
      "    fold 9: [0.80461747]\n",
      "    ----\n",
      "    MEAN:   [0.81501109]\n",
      "\n",
      "model 6: [ElasticNet]\n",
      "    fold 0: [0.89904515]\n",
      "    fold 1: [0.88881226]\n",
      "    fold 2: [0.87984474]\n",
      "    fold 3: [0.87073242]\n",
      "    fold 4: [0.85813890]\n",
      "    fold 5: [0.86031598]\n",
      "    fold 6: [0.47696048]\n",
      "    fold 7: [0.74599437]\n",
      "    fold 8: [0.81748847]\n",
      "    fold 9: [0.79909940]\n",
      "    ----\n",
      "    MEAN:   [0.81970670]\n",
      "\n",
      "model 7: [BayesianRidge]\n",
      "    fold 0: [0.90461652]\n",
      "    fold 1: [0.89460976]\n",
      "    fold 2: [0.87191725]\n",
      "    fold 3: [0.89051027]\n",
      "    fold 4: [0.84990061]\n",
      "    fold 5: [0.84941556]\n",
      "    fold 6: [0.39721969]\n",
      "    fold 7: [0.71542795]\n",
      "    fold 8: [0.84260531]\n",
      "    fold 9: [0.76053057]\n",
      "    ----\n",
      "    MEAN:   [0.81245877]\n",
      "\n",
      "model 8: [Lasso]\n",
      "    fold 0: [0.85473208]\n",
      "    fold 1: [0.86402785]\n",
      "    fold 2: [0.83498911]\n",
      "    fold 3: [0.87526622]\n",
      "    fold 4: [0.81970385]\n",
      "    fold 5: [0.87693424]\n",
      "    fold 6: [0.34528293]\n",
      "    fold 7: [0.73031797]\n",
      "    fold 8: [0.79006299]\n",
      "    fold 9: [0.83714738]\n",
      "    ----\n",
      "    MEAN:   [0.79650951]\n",
      "\n",
      "model 9: [LassoCV]\n",
      "    fold 0: [0.88868340]\n",
      "    fold 1: [0.86305376]\n",
      "    fold 2: [0.85698176]\n",
      "    fold 3: [0.85977473]\n",
      "    fold 4: [0.82940589]\n",
      "    fold 5: [0.86248567]\n",
      "    fold 6: [0.44042419]\n",
      "    fold 7: [0.74400792]\n",
      "    fold 8: [0.79739807]\n",
      "    fold 9: [0.85428741]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 117 iterations, i.e. alpha=2.210e+01, with an active set of 105 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    ----\n",
      "    MEAN:   [0.80899250]\n",
      "\n",
      "model 10: [LassoLars]\n",
      "    fold 0: [0.74278155]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 170 iterations, i.e. alpha=1.105e+01, with an active set of 152 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 205 iterations, i.e. alpha=6.120e+00, with an active set of 175 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 218 iterations, alpha=5.373e+00, previous alpha=5.346e+00, with an active set of 187 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.840e+01, with an active set of 117 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 1: [0.89354367]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 133 iterations, i.e. alpha=1.714e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=1.642e+01, with an active set of 127 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 137 iterations, alpha=1.550e+01, previous alpha=1.544e+01, with an active set of 128 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 106 iterations, i.e. alpha=2.302e+01, with an active set of 102 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 2: [0.86244668]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 119 iterations, i.e. alpha=2.075e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 119 iterations, i.e. alpha=2.075e+01, with an active set of 115 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.883e+01, with an active set of 123 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 129 iterations, i.e. alpha=1.750e+01, with an active set of 125 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 135 iterations, i.e. alpha=1.636e+01, with an active set of 131 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 142 iterations, alpha=1.551e+01, previous alpha=1.532e+01, with an active set of 135 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 128 iterations, i.e. alpha=1.736e+01, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 3: [0.90470695]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 160 iterations, i.e. alpha=1.097e+01, with an active set of 152 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 177 iterations, i.e. alpha=8.910e+00, with an active set of 165 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=8.682e+00, with an active set of 168 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 182 iterations, i.e. alpha=8.682e+00, with an active set of 168 regressors, and the smallest cholesky pivot element being 3.942e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 185 iterations, i.e. alpha=8.165e+00, with an active set of 171 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 187 iterations, i.e. alpha=7.793e+00, with an active set of 173 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 187 iterations, i.e. alpha=7.675e+00, with an active set of 173 regressors, and the smallest cholesky pivot element being 3.161e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 192 iterations, alpha=7.829e+00, previous alpha=7.426e+00, with an active set of 177 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=1.527e+01, with an active set of 128 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 4: [0.79530016]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 143 iterations, i.e. alpha=1.472e+01, with an active set of 129 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 165 iterations, i.e. alpha=9.937e+00, with an active set of 149 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 169 iterations, i.e. alpha=8.985e+00, with an active set of 153 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=7.635e+00, with an active set of 158 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 181 iterations, i.e. alpha=6.998e+00, with an active set of 163 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 193 iterations, alpha=6.296e+00, previous alpha=6.227e+00, with an active set of 172 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 79 iterations, i.e. alpha=3.607e+01, with an active set of 77 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 5: [0.82618345]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 103 iterations, i.e. alpha=2.668e+01, with an active set of 99 regressors, and the smallest cholesky pivot element being 2.980e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 112 iterations, alpha=2.430e+01, previous alpha=2.424e+01, with an active set of 107 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 118 iterations, i.e. alpha=1.913e+01, with an active set of 110 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 6: [0.37438420]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 147 iterations, i.e. alpha=1.160e+01, with an active set of 135 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=1.048e+01, with an active set of 142 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 154 iterations, i.e. alpha=1.048e+01, with an active set of 142 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 163 iterations, i.e. alpha=9.528e+00, with an active set of 149 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=5.198e+00, with an active set of 189 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 208 iterations, i.e. alpha=4.969e+00, with an active set of 190 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 210 iterations, i.e. alpha=4.725e+00, with an active set of 192 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 214 iterations, i.e. alpha=4.388e+00, with an active set of 196 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 220 iterations, i.e. alpha=4.026e+00, with an active set of 200 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 229 iterations, alpha=3.772e+00, previous alpha=3.558e+00, with an active set of 208 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 93 iterations, i.e. alpha=3.255e+01, with an active set of 87 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 7: [0.65831684]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 125 iterations, i.e. alpha=1.923e+01, with an active set of 113 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 134 iterations, i.e. alpha=1.672e+01, with an active set of 122 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 139 iterations, i.e. alpha=1.605e+01, with an active set of 127 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 149 iterations, alpha=1.406e+01, previous alpha=1.403e+01, with an active set of 136 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 126 iterations, i.e. alpha=1.666e+01, with an active set of 122 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 8: [0.87005596]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 127 iterations, i.e. alpha=1.665e+01, with an active set of 123 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 140 iterations, i.e. alpha=1.442e+01, with an active set of 134 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 162 iterations, i.e. alpha=1.064e+01, with an active set of 152 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 183 iterations, i.e. alpha=8.332e+00, with an active set of 167 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 183 iterations, i.e. alpha=8.317e+00, with an active set of 167 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 183 iterations, i.e. alpha=8.198e+00, with an active set of 167 regressors, and the smallest cholesky pivot element being 2.356e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 186 iterations, alpha=8.155e+00, previous alpha=8.042e+00, with an active set of 169 regressors.\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 72 iterations, i.e. alpha=4.492e+01, with an active set of 70 regressors, and the smallest cholesky pivot element being 2.788e-08\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    fold 9: [0.70077752]\n",
      "    ----\n",
      "    MEAN:   [0.78469916]\n",
      "\n",
      "model 11: [Ridge]\n",
      "    fold 0: [0.52694459]\n",
      "    fold 1: [0.66256973]\n",
      "    fold 2: [0.41315969]\n",
      "    fold 3: [0.91204567]\n",
      "    fold 4: [0.79817002]\n",
      "    fold 5: [0.89290129]\n",
      "    fold 6: [0.42787744]\n",
      "    fold 7: [0.59076862]\n",
      "    fold 8: [0.85507136]\n",
      "    fold 9: [0.64089062]\n",
      "    ----\n",
      "    MEAN:   [0.70547843]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 107 iterations, i.e. alpha=2.508e+01, with an active set of 103 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 153 iterations, i.e. alpha=1.357e+01, with an active set of 137 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 158 iterations, i.e. alpha=1.322e+01, with an active set of 142 regressors, and the smallest cholesky pivot element being 1.054e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 176 iterations, i.e. alpha=1.073e+01, with an active set of 152 regressors, and the smallest cholesky pivot element being 1.825e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 178 iterations, i.e. alpha=1.036e+01, with an active set of 154 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 200 iterations, i.e. alpha=7.843e+00, with an active set of 172 regressors, and the smallest cholesky pivot element being 2.107e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 200 iterations, i.e. alpha=7.843e+00, with an active set of 172 regressors, and the smallest cholesky pivot element being 2.220e-16\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 206 iterations, i.e. alpha=6.749e+00, with an active set of 178 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 207 iterations, i.e. alpha=6.539e+00, with an active set of 179 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 210 iterations, i.e. alpha=6.019e+00, with an active set of 182 regressors, and the smallest cholesky pivot element being 1.490e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:309: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 212 iterations, i.e. alpha=5.871e+00, with an active set of 184 regressors, and the smallest cholesky pivot element being 2.581e-08\n",
      "  ConvergenceWarning)\n",
      "/home/rakesh/anaconda/lib/python2.7/site-packages/sklearn/linear_model/least_angle.py:334: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 216 iterations, alpha=5.868e+00, previous alpha=5.540e+00, with an active set of 187 regressors.\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "S_train, S_test = stacking(base_models, X_train.values, y_train, X_test.values, regression=True, metric=r2_score,\n",
    "                           n_folds=10, shuffle=True, random_state=10, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_train = pd.DataFrame(S_train)\n",
    "S_test = pd.DataFrame(S_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.965077</td>\n",
       "      <td>0.947201</td>\n",
       "      <td>0.981238</td>\n",
       "      <td>0.953121</td>\n",
       "      <td>0.957062</td>\n",
       "      <td>0.951760</td>\n",
       "      <td>0.947522</td>\n",
       "      <td>0.938456</td>\n",
       "      <td>0.946760</td>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.896941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.965077</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990538</td>\n",
       "      <td>0.952449</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>0.931328</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>0.915383</td>\n",
       "      <td>0.908127</td>\n",
       "      <td>0.913955</td>\n",
       "      <td>0.903717</td>\n",
       "      <td>0.871462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.947201</td>\n",
       "      <td>0.990538</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.936943</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.913667</td>\n",
       "      <td>0.907911</td>\n",
       "      <td>0.904416</td>\n",
       "      <td>0.896951</td>\n",
       "      <td>0.901433</td>\n",
       "      <td>0.891399</td>\n",
       "      <td>0.861476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.981238</td>\n",
       "      <td>0.952449</td>\n",
       "      <td>0.936943</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.944956</td>\n",
       "      <td>0.971147</td>\n",
       "      <td>0.952984</td>\n",
       "      <td>0.947436</td>\n",
       "      <td>0.939521</td>\n",
       "      <td>0.951865</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>0.899312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.953121</td>\n",
       "      <td>0.991875</td>\n",
       "      <td>0.998635</td>\n",
       "      <td>0.944956</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.920712</td>\n",
       "      <td>0.914185</td>\n",
       "      <td>0.910460</td>\n",
       "      <td>0.903443</td>\n",
       "      <td>0.908091</td>\n",
       "      <td>0.898758</td>\n",
       "      <td>0.868973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.957062</td>\n",
       "      <td>0.931328</td>\n",
       "      <td>0.913667</td>\n",
       "      <td>0.971147</td>\n",
       "      <td>0.920712</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922930</td>\n",
       "      <td>0.917693</td>\n",
       "      <td>0.910878</td>\n",
       "      <td>0.926922</td>\n",
       "      <td>0.911023</td>\n",
       "      <td>0.872576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.951760</td>\n",
       "      <td>0.919014</td>\n",
       "      <td>0.907911</td>\n",
       "      <td>0.952984</td>\n",
       "      <td>0.914185</td>\n",
       "      <td>0.922930</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.996233</td>\n",
       "      <td>0.980765</td>\n",
       "      <td>0.982295</td>\n",
       "      <td>0.979110</td>\n",
       "      <td>0.929017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.947522</td>\n",
       "      <td>0.915383</td>\n",
       "      <td>0.904416</td>\n",
       "      <td>0.947436</td>\n",
       "      <td>0.910460</td>\n",
       "      <td>0.917693</td>\n",
       "      <td>0.996233</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.980309</td>\n",
       "      <td>0.975933</td>\n",
       "      <td>0.986984</td>\n",
       "      <td>0.943466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.938456</td>\n",
       "      <td>0.908127</td>\n",
       "      <td>0.896951</td>\n",
       "      <td>0.939521</td>\n",
       "      <td>0.903443</td>\n",
       "      <td>0.910878</td>\n",
       "      <td>0.980765</td>\n",
       "      <td>0.980309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.984748</td>\n",
       "      <td>0.966085</td>\n",
       "      <td>0.939581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.946760</td>\n",
       "      <td>0.913955</td>\n",
       "      <td>0.901433</td>\n",
       "      <td>0.951865</td>\n",
       "      <td>0.908091</td>\n",
       "      <td>0.926922</td>\n",
       "      <td>0.982295</td>\n",
       "      <td>0.975933</td>\n",
       "      <td>0.984748</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.961291</td>\n",
       "      <td>0.912513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.935264</td>\n",
       "      <td>0.903717</td>\n",
       "      <td>0.891399</td>\n",
       "      <td>0.937440</td>\n",
       "      <td>0.898758</td>\n",
       "      <td>0.911023</td>\n",
       "      <td>0.979110</td>\n",
       "      <td>0.986984</td>\n",
       "      <td>0.966085</td>\n",
       "      <td>0.961291</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.954658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.896941</td>\n",
       "      <td>0.871462</td>\n",
       "      <td>0.861476</td>\n",
       "      <td>0.899312</td>\n",
       "      <td>0.868973</td>\n",
       "      <td>0.872576</td>\n",
       "      <td>0.929017</td>\n",
       "      <td>0.943466</td>\n",
       "      <td>0.939581</td>\n",
       "      <td>0.912513</td>\n",
       "      <td>0.954658</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   1.000000  0.965077  0.947201  0.981238  0.953121  0.957062  0.951760   \n",
       "1   0.965077  1.000000  0.990538  0.952449  0.991875  0.931328  0.919014   \n",
       "2   0.947201  0.990538  1.000000  0.936943  0.998635  0.913667  0.907911   \n",
       "3   0.981238  0.952449  0.936943  1.000000  0.944956  0.971147  0.952984   \n",
       "4   0.953121  0.991875  0.998635  0.944956  1.000000  0.920712  0.914185   \n",
       "5   0.957062  0.931328  0.913667  0.971147  0.920712  1.000000  0.922930   \n",
       "6   0.951760  0.919014  0.907911  0.952984  0.914185  0.922930  1.000000   \n",
       "7   0.947522  0.915383  0.904416  0.947436  0.910460  0.917693  0.996233   \n",
       "8   0.938456  0.908127  0.896951  0.939521  0.903443  0.910878  0.980765   \n",
       "9   0.946760  0.913955  0.901433  0.951865  0.908091  0.926922  0.982295   \n",
       "10  0.935264  0.903717  0.891399  0.937440  0.898758  0.911023  0.979110   \n",
       "11  0.896941  0.871462  0.861476  0.899312  0.868973  0.872576  0.929017   \n",
       "\n",
       "          7         8         9         10        11  \n",
       "0   0.947522  0.938456  0.946760  0.935264  0.896941  \n",
       "1   0.915383  0.908127  0.913955  0.903717  0.871462  \n",
       "2   0.904416  0.896951  0.901433  0.891399  0.861476  \n",
       "3   0.947436  0.939521  0.951865  0.937440  0.899312  \n",
       "4   0.910460  0.903443  0.908091  0.898758  0.868973  \n",
       "5   0.917693  0.910878  0.926922  0.911023  0.872576  \n",
       "6   0.996233  0.980765  0.982295  0.979110  0.929017  \n",
       "7   1.000000  0.980309  0.975933  0.986984  0.943466  \n",
       "8   0.980309  1.000000  0.984748  0.966085  0.939581  \n",
       "9   0.975933  0.984748  1.000000  0.961291  0.912513  \n",
       "10  0.986984  0.966085  0.961291  1.000000  0.954658  \n",
       "11  0.943466  0.939581  0.912513  0.954658  1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = S_train.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f541bf8e410>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWYAAAEDCAYAAAAGH67/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ2NTlU0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f544127de10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_train_mod = S_train.iloc[:, [0,11]]\n",
    "S_test_mod = S_test.iloc[:, [0,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0         11\n",
      "0   1.000000  0.896941\n",
      "11  0.896941  1.000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f5439e111d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWEAAAEDCAYAAADkw7WGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5439cc9a90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = S_train_mod.corr()\n",
    "print corr\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blend_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_train_mod, S_test_mod = standardize(S_train_mod, S_test_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "S_train_new = pd.concat([X_train, S_train_mod], axis=1)\n",
    "S_test_new = pd.concat([X_test, S_test_mod], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report\n",
      "Train_coef_of_det: 0.935751\n",
      "CV_coef_of_det: Mean-0.891372 | Std-0.033337 | Min-0.826268 | Max-0.931707\n"
     ]
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "modelfit(model, S_train_mod, np.log(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_pred = np.exp(model.predict(S_test_mod))\n",
    "dictn = {'Id': range(1461,2920), 'SalePrice': test_pred}\n",
    "res = pd.DataFrame(dictn)\n",
    "res.to_csv('submission.csv', index=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py27",
   "language": "python",
   "name": "py27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
